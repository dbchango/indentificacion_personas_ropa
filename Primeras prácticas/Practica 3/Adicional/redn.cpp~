#include "redn.h"

int Red::read_data_from_csv(const char* filename, Mat data, Mat classes, int n_samples )
{

    int classlabel; // the class label
    float tmpf;
    int fs;

    // if we can't read the input file then return 0
    FILE* f = fopen( filename, "r" );
    if( !f )
    {
        printf("ERROR: cannot read file %s\n",  filename);
        return 0; // all not OK
    }

    // for each sample in the file

    for(int line = 0; line < n_samples; line++)
    {

        // for each attribute on the line in the file

        for(int attribute = 0; attribute < (ATTRIBUTES_PER_SAMPLE + 1); attribute++)
        {
            if (attribute < ATTRIBUTES_PER_SAMPLE)
            {
                // first 3 elements (0-2) in each line are the attributes

                fscanf(f, "%f,", &tmpf);
                data.at<float>(line, attribute) = tmpf;

            }
            else if (attribute == ATTRIBUTES_PER_SAMPLE)
            {
                // attribute 4 is the class label {0 ... 2}

                fscanf(f, "%i", &classlabel);
                classes.at<float>(line, classlabel) = 1.0;
            }
        }
    }

    fclose(f);

    return 1; // all OK
}


int Red::test_network(){

   // define testing data storage matrices

   Mat testing_data = Mat(NUMBER_OF_TESTING_SAMPLES, ATTRIBUTES_PER_SAMPLE, CV_32FC1);
   Mat testing_classifications = Mat::zeros(NUMBER_OF_TESTING_SAMPLES, NUMBER_OF_CLASSES, CV_32FC1);

   // define classification output vector

   Mat classificationResult = Mat(1, NUMBER_OF_CLASSES, CV_32FC1);
   Point max_loc = Point(0,0);

   if (read_data_from_csv("data1.txt", testing_data, testing_classifications, NUMBER_OF_TESTING_SAMPLES))
   {

      // perform classifier testing and report results

      Mat test_sample;
      int correct_class = 0;
      int wrong_class = 0;
      int false_positives [NUMBER_OF_CLASSES] = {0,0,0};

      // extract a row from the testing matrix

      test_sample = testing_data.row(0);

      // run neural network prediction

      nnetwork->predict(test_sample, classificationResult);

      // The NN gives out a vector of probabilities for each class
      // We take the class with the highest "probability"
      // for simplicity (but we really should also check separation
      // of the different "probabilities" in this vector - what if
      // two classes have very similar values ?)

      minMaxLoc(classificationResult, 0, 0, 0, &max_loc);

      // if the corresponding location in the testing classifications
      // is not "1" (i.e. this is the correct class) then record this

      if (!(testing_classifications.at<float>(0, max_loc.x)))
      {
         // if they differ more than floating point error => wrong class
         wrong_class++;
         false_positives[(int) max_loc.x]++;
      }
      else
      {
         // otherwise correct
         correct_class++;
      }
      return max_loc.x;
   }
   return -1;

}



int Red::training_network()
{
      printf("HOLAAA \n");	

      // define training data storage matrices (one for attribute examples, one
      // for classifications)

      Mat training_data = Mat(NUMBER_OF_TRAINING_SAMPLES, ATTRIBUTES_PER_SAMPLE, CV_32FC1);
      Mat training_classifications = Mat(NUMBER_OF_TRAINING_SAMPLES, NUMBER_OF_CLASSES, CV_32FC1);

      nnetwork = new CvANN_MLP;

      // load training and testing data sets

      if (read_data_from_csv("ent.txt", training_data, training_classifications, NUMBER_OF_TRAINING_SAMPLES))
      {
         // define the parameters for the neural network (MLP)

         // set the network to be 3 layer 256->10->10
         // - one input node per attribute in a sample
         // - 10 hidden nodes
         // - one output node per class

         // note that the OpenCV neural network (MLP) implementation does not
         // support categorical variables explicitly.
         // So, instead of the output class label, we will use
         // a binary vector of {0,0 ... 1,0,0} components (one element by class)
         // for training and therefore, MLP will give us a vector of "probabilities"
         // at the prediction stage - the highest probability can be accepted
         // as the "winning" class label output by the network

         int layers_d[] = { ATTRIBUTES_PER_SAMPLE, 10,  NUMBER_OF_CLASSES};
         Mat layers = Mat(1,3,CV_32SC1);
         layers.at<int>(0,0) = layers_d[0];
         layers.at<int>(0,1) = layers_d[1];
         layers.at<int>(0,2) = layers_d[2];

         // create the network using a sigmoid function with alpha and beta
         // parameters 0.6 and 1 specified respectively (refer to manual)
      
         nnetwork->create(layers, CvANN_MLP::SIGMOID_SYM, 0.6, 1);

         // set the training parameters

         CvANN_MLP_TrainParams params = CvANN_MLP_TrainParams(
                                    // terminate the training after either 1000
                                    // iterations or a very small change in the
                                    // network wieghts below the specified value

                                    cvTermCriteria(CV_TERMCRIT_ITER+CV_TERMCRIT_EPS, 1000, 0.000001),

                                    // use backpropogation for training

                                    CvANN_MLP_TrainParams::BACKPROP,

                                    // co-efficents for backpropogation training (refer to manual)

                                    0.1, 0.1);

         // train the neural network (using training data)

         int iterations = nnetwork->train(training_data, training_classifications, Mat(), Mat(), params);

         num = 1;
	 return 0; //All Ok
      }

    return -1; //All Bad
}
/******************************************************************************/

